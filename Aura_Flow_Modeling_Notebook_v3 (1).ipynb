{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a2a9e8",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ©¸ Aura Flow: Menstrual Health Modeling Notebook ðŸ©¸\n",
    "## **Predicting Period Pain Levels with Machine Learning**\n",
    "This notebook builds **multiple machine learning models** to classify period pain levels, ensuring continuity with the **preprocessed dataset** from the previous analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "0a22b2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>Finished</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>DistributionChannel</th>\n",
       "      <th>UserLanguage</th>\n",
       "      <th>Age_Group</th>\n",
       "      <th>...</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>Q41</th>\n",
       "      <th>Q43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-07 17:11:00</td>\n",
       "      <td>2023-02-07 17:18:00</td>\n",
       "      <td>Imported</td>\n",
       "      <td>100</td>\n",
       "      <td>410</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-23 08:38:00</td>\n",
       "      <td>qr</td>\n",
       "      <td>EN</td>\n",
       "      <td>22-23</td>\n",
       "      <td>...</td>\n",
       "      <td>Dissatisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Quite often</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-08 12:20:00</td>\n",
       "      <td>2023-02-08 12:25:00</td>\n",
       "      <td>Imported</td>\n",
       "      <td>100</td>\n",
       "      <td>334</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-23 08:38:00</td>\n",
       "      <td>qr</td>\n",
       "      <td>EN</td>\n",
       "      <td>20-21</td>\n",
       "      <td>...</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Quite often</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-07 15:20:00</td>\n",
       "      <td>2023-02-07 15:26:00</td>\n",
       "      <td>Imported</td>\n",
       "      <td>100</td>\n",
       "      <td>382</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-23 08:38:00</td>\n",
       "      <td>qr</td>\n",
       "      <td>EN</td>\n",
       "      <td>18-19</td>\n",
       "      <td>...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Seldom</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-08 13:13:00</td>\n",
       "      <td>2023-02-08 13:17:00</td>\n",
       "      <td>Imported</td>\n",
       "      <td>100</td>\n",
       "      <td>288</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-23 08:38:00</td>\n",
       "      <td>qr</td>\n",
       "      <td>EN</td>\n",
       "      <td>22-23</td>\n",
       "      <td>...</td>\n",
       "      <td>Dissatisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Dissatisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very often</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-07 19:17:00</td>\n",
       "      <td>2023-02-07 19:25:00</td>\n",
       "      <td>Imported</td>\n",
       "      <td>100</td>\n",
       "      <td>464</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-23 08:38:00</td>\n",
       "      <td>qr</td>\n",
       "      <td>EN</td>\n",
       "      <td>20-21</td>\n",
       "      <td>...</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>Seldom</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartDate              EndDate    Status  Progress  \\\n",
       "0  2023-02-07 17:11:00  2023-02-07 17:18:00  Imported       100   \n",
       "1  2023-02-08 12:20:00  2023-02-08 12:25:00  Imported       100   \n",
       "2  2023-02-07 15:20:00  2023-02-07 15:26:00  Imported       100   \n",
       "3  2023-02-08 13:13:00  2023-02-08 13:17:00  Imported       100   \n",
       "4  2023-02-07 19:17:00  2023-02-07 19:25:00  Imported       100   \n",
       "\n",
       "   Duration (in seconds)  Finished         RecordedDate DistributionChannel  \\\n",
       "0                    410      True  2023-03-23 08:38:00                  qr   \n",
       "1                    334      True  2023-03-23 08:38:00                  qr   \n",
       "2                    382      True  2023-03-23 08:38:00                  qr   \n",
       "3                    288      True  2023-03-23 08:38:00                  qr   \n",
       "4                    464      True  2023-03-23 08:38:00                  qr   \n",
       "\n",
       "  UserLanguage Age_Group  ...              42              43              44  \\\n",
       "0           EN     22-23  ...    Dissatisfied       Satisfied       Satisfied   \n",
       "1           EN     20-21  ...  Very satisfied  Very satisfied       Satisfied   \n",
       "2           EN     18-19  ...       Satisfied       Satisfied  Very satisfied   \n",
       "3           EN     22-23  ...    Dissatisfied       Satisfied    Dissatisfied   \n",
       "4           EN     20-21  ...  Very satisfied  Very satisfied  Very satisfied   \n",
       "\n",
       "               45              46              47              48  \\\n",
       "0       Satisfied       Satisfied  Very satisfied  Very satisfied   \n",
       "1       Satisfied       Satisfied       Satisfied  Very satisfied   \n",
       "2  Very satisfied  Very satisfied       Satisfied       Satisfied   \n",
       "3       Satisfied       Satisfied  Very satisfied  Very satisfied   \n",
       "4  Very satisfied       Satisfied  Very satisfied  Very satisfied   \n",
       "\n",
       "            49  Q41  Q43  \n",
       "0  Quite often  Yes  Yes  \n",
       "1  Quite often  Yes  Yes  \n",
       "2       Seldom   No   No  \n",
       "3   Very often   No   No  \n",
       "4       Seldom  Yes  Yes  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load Preprocessed Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset \n",
    "file_path = \"cleaned_menstrual_health_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "3a9a88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Define Features and Target Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define target variable\n",
    "target = \"Period_Pain_Level\"\n",
    "\n",
    "\n",
    "features = [\"Work_School_Impact_Days\", \"Concern_Level_About_Bleeding\", \n",
    "            \"Personal_Income\", \"Missed_Work_School_Days\", \"Heavy_Bleeding_Indicators\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data successfully split into training and testing sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "50bb97ce-0817-4298-8338-d1dd071c6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work_School_Impact_Days         object\n",
      "Concern_Level_About_Bleeding    object\n",
      "Personal_Income                 object\n",
      "Missed_Work_School_Days         object\n",
      "Heavy_Bleeding_Indicators       object\n",
      "dtype: object\n",
      "\n",
      "Non-numeric columns: Index(['Work_School_Impact_Days', 'Concern_Level_About_Bleeding',\n",
      "       'Personal_Income', 'Missed_Work_School_Days',\n",
      "       'Heavy_Bleeding_Indicators'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check data types of features\n",
    "print(X_train.dtypes)\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nNon-numeric columns:\", non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "533262bb-54d4-459f-b3f9-3a150f81da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in Work_School_Impact_Days:\n",
      "['1-3 days' '4-8 days'\n",
      " 'Never, my bleeding does not affect my work or school' '9-12 days'\n",
      " 'I am currently not working or attending school outside of the home'\n",
      " '13 days or more']\n",
      "\n",
      "Unique values in Concern_Level_About_Bleeding:\n",
      "['2' '3' '4' '0' '5' '7' '1' '6' '10' '8' '9' 'Unknown']\n",
      "\n",
      "Unique values in Personal_Income:\n",
      "['20,000-34,999' 'Under 20,000' '35,000- 49,999' '75,000-99,999'\n",
      " '50,000-74,999' 'Over 100,000' 'Unknown']\n",
      "\n",
      "Unique values in Missed_Work_School_Days:\n",
      "['Never, my bleeding does not affect my work or school' '1-3 days'\n",
      " '4-8 days' '13 days or more'\n",
      " 'I am currently not working or attending school outside of the home'\n",
      " '9-12 days']\n",
      "\n",
      "Unique values in Heavy_Bleeding_Indicators:\n",
      "['Need to use double sanitary protection to control your menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Unknown'\n",
      " 'Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week'\n",
      " 'Need to wake up to change sanitary protection during the night,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter'\n",
      " 'Need to wake up to change sanitary protection during the night,Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours'\n",
      " 'Need to wake up to change sanitary protection during the night'\n",
      " 'Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Bleed for longer than a week'\n",
      " 'Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Pass blood clots larger than a quarter'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Bleed for longer than a week'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to use double sanitary protection to control your menstrual flow,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Bleed for longer than a week,Pass blood clots larger than a quarter'\n",
      " 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Need to wake up to change sanitary protection during the night,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow']\n"
     ]
    }
   ],
   "source": [
    "# Display unique values for each categorical column\n",
    "for col in ['Work_School_Impact_Days', 'Concern_Level_About_Bleeding', \n",
    "            'Personal_Income', 'Missed_Work_School_Days', 'Heavy_Bleeding_Indicators']:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(df[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "977cf0a7-d82d-4c37-b24b-48436266626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define mappings\n",
    "work_school_mapping = {\n",
    "    \"Never, my bleeding does not affect my work or school\": 0,\n",
    "    \"I am currently not working or attending school outside of the home\": -1,  # Special case\n",
    "    \"1-3 days\": 2,\n",
    "    \"4-8 days\": 6,\n",
    "    \"9-12 days\": 10.5,\n",
    "    \"13 days or more\": 13\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df[\"Work_School_Impact_Days\"] = df[\"Work_School_Impact_Days\"].map(work_school_mapping)\n",
    "df[\"Missed_Work_School_Days\"] = df[\"Missed_Work_School_Days\"].map(work_school_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "8551f722-8495-420a-9fbd-74be00483a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Concern_Level_About_Bleeding\"] = df[\"Concern_Level_About_Bleeding\"].replace(\"Unknown\", np.nan).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "5b3a95b0-52d6-4cd9-a5d5-899a0306c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_mapping = {\n",
    "    \"Under 20,000\": 10000,\n",
    "    \"20,000-34,999\": 27500,\n",
    "    \"35,000- 49,999\": 42500,\n",
    "    \"50,000-74,999\": 62500,\n",
    "    \"75,000-99,999\": 87500,\n",
    "    \"Over 100,000\": 110000,\n",
    "    \"Unknown\": np.nan\n",
    "}\n",
    "\n",
    "df[\"Personal_Income\"] = df[\"Personal_Income\"].map(income_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "2aa8ac3c-318c-494a-88e2-982b448c8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of symptoms to extract\n",
    "symptoms = [\n",
    "    \"Need to use double sanitary protection\",\n",
    "    \"Soak through one or more sanitary pads or tampons every hour\",\n",
    "    \"Bleed for longer than a week\",\n",
    "    \"Pass blood clots larger than a quarter\",\n",
    "    \"Restrict daily activities due to heavy menstrual flow\",\n",
    "    \"Need to wake up to change sanitary protection during the night\"\n",
    "]\n",
    "\n",
    "\n",
    "for symptom in symptoms:\n",
    "    df[symptom] = df[\"Heavy_Bleeding_Indicators\"].apply(lambda x: 1 if symptom in str(x) else 0)\n",
    "\n",
    "\n",
    "df.drop(columns=[\"Heavy_Bleeding_Indicators\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "3d933568-61b6-4a00-9473-e76e69906f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartDate                                                         object\n",
      "EndDate                                                           object\n",
      "Status                                                            object\n",
      "Progress                                                           int64\n",
      "Duration (in seconds)                                              int64\n",
      "                                                                   ...  \n",
      "Soak through one or more sanitary pads or tampons every hour       int64\n",
      "Bleed for longer than a week                                       int64\n",
      "Pass blood clots larger than a quarter                             int64\n",
      "Restrict daily activities due to heavy menstrual flow              int64\n",
      "Need to wake up to change sanitary protection during the night     int64\n",
      "Length: 71, dtype: object\n",
      "StartDate                                                         0\n",
      "EndDate                                                           0\n",
      "Status                                                            0\n",
      "Progress                                                          0\n",
      "Duration (in seconds)                                             0\n",
      "                                                                 ..\n",
      "Soak through one or more sanitary pads or tampons every hour      0\n",
      "Bleed for longer than a week                                      0\n",
      "Pass blood clots larger than a quarter                            0\n",
      "Restrict daily activities due to heavy menstrual flow             0\n",
      "Need to wake up to change sanitary protection during the night    0\n",
      "Length: 71, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)  \n",
    "print(df.isnull().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "abeb50c4-4aa2-4d03-a2bc-0170dfee7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnecessary columns removed!\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary text-based columns\n",
    "df = df.drop(columns=[\"StartDate\", \"EndDate\", \"Status\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Unnecessary columns removed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "6b2ce3b4-063f-48d6-860f-e8f6062e2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Non-Numeric Columns: Index(['Finished', 'RecordedDate', 'DistributionChannel', 'UserLanguage',\n",
      "       'Age_Group', 'Gender_Identity', 'Q68', 'Race', 'Health_Insurance',\n",
      "       'Primary_Insurance', '5_4_TEXT', 'Period_Flow_Description',\n",
      "       'Period_Pain_Level', 'Avoided_Social_Activities', 'Chronic_Pelvic_Pain',\n",
      "       'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', '17', '19',\n",
      "       '20', '21', '22', '23', '24', '25', '27', '28', '29', '30', '31', '32',\n",
      "       '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44',\n",
      "       '45', '46', '47', '48', '49', 'Q41', 'Q43'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find any remaining non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "print(\"Remaining Non-Numeric Columns:\", non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "5a98826f-67e1-4c20-8040-576d70889ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Columns in DataFrame:\n",
      "['Progress', 'Duration (in seconds)', 'Finished', 'RecordedDate', 'DistributionChannel', 'UserLanguage', 'Age_Group', 'Gender_Identity', 'Q68', 'Race', 'Health_Insurance', 'Primary_Insurance', '5_4_TEXT', 'Personal_Income', 'Period_Flow_Description', 'Period_Pain_Level', 'Work_School_Impact_Days', 'Missed_Work_School_Days', 'Avoided_Social_Activities', 'Concern_Level_About_Bleeding', 'Chronic_Pelvic_Pain', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', '17', '19', '20', '21', '22', '23', '24', '25', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', 'Q41', 'Q43', 'Need to use double sanitary protection', 'Soak through one or more sanitary pads or tampons every hour', 'Bleed for longer than a week', 'Pass blood clots larger than a quarter', 'Restrict daily activities due to heavy menstrual flow', 'Need to wake up to change sanitary protection during the night']\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Columns in DataFrame:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "ba793711-d460-4340-b931-e4a95186f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress                                                           int64\n",
      "Duration (in seconds)                                              int64\n",
      "Finished                                                            bool\n",
      "RecordedDate                                                      object\n",
      "DistributionChannel                                               object\n",
      "                                                                   ...  \n",
      "Soak through one or more sanitary pads or tampons every hour       int64\n",
      "Bleed for longer than a week                                       int64\n",
      "Pass blood clots larger than a quarter                             int64\n",
      "Restrict daily activities due to heavy menstrual flow              int64\n",
      "Need to wake up to change sanitary protection during the night     int64\n",
      "Length: 68, dtype: object\n",
      "Progress                                                          0\n",
      "Duration (in seconds)                                             0\n",
      "Finished                                                          0\n",
      "RecordedDate                                                      0\n",
      "DistributionChannel                                               0\n",
      "                                                                 ..\n",
      "Soak through one or more sanitary pads or tampons every hour      0\n",
      "Bleed for longer than a week                                      0\n",
      "Pass blood clots larger than a quarter                            0\n",
      "Restrict daily activities due to heavy menstrual flow             0\n",
      "Need to wake up to change sanitary protection during the night    0\n",
      "Length: 68, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check column data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "ba7accc8-6cd7-48dd-93dc-a623bc875d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Numeric Columns: ['RecordedDate', 'DistributionChannel', 'UserLanguage', 'Age_Group', 'Gender_Identity', 'Q68', 'Race', 'Health_Insurance', 'Primary_Insurance', '5_4_TEXT', 'Period_Flow_Description', 'Period_Pain_Level', 'Avoided_Social_Activities', 'Chronic_Pelvic_Pain', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', '17', '19', '20', '21', '22', '23', '24', '25', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', 'Q41', 'Q43']\n"
     ]
    }
   ],
   "source": [
    "# Find remaining object columns\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"Non-Numeric Columns:\", non_numeric_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "eb4cb514-cb0b-4ace-9a2a-0810e1198f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed unnecessary columns.\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns that don't help prediction\n",
    "drop_cols = [\"RecordedDate\", \"5_4_TEXT\", \"Q68\"]\n",
    "df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Removed unnecessary columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "da270c18-782a-4a31-9847-55d21ecb435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted categorical columns to numeric values.\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variables into numbers\n",
    "categorical_cols = [\n",
    "    \"DistributionChannel\", \"UserLanguage\", \"Age_Group\", \"Gender_Identity\", \"Race\",\n",
    "    \"Health_Insurance\", \"Primary_Insurance\", \"Period_Flow_Description\"\n",
    "]\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Converted categorical columns to numeric values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "0e0b1c82-94ac-42bf-8712-5dde6ace5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Yes/No columns to binary (1/0).\n"
     ]
    }
   ],
   "source": [
    "# Convert Yes/No answers to binary (1/0)\n",
    "binary_cols = [\"Avoided_Social_Activities\", \"Chronic_Pelvic_Pain\"]\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "print(\"Converted Yes/No columns to binary (1/0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "b36f58fc-f9f1-451b-86a0-280642a247d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted survey response columns to numbers.\n"
     ]
    }
   ],
   "source": [
    "# Convert survey responses to numeric (if they are not already)\n",
    "survey_cols = [col for col in df.columns if col.startswith(\"Q\") or col.isdigit()]\n",
    "df[survey_cols] = df[survey_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "print(\"Converted survey response columns to numbers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "414b807c-9048-4257-ac16-27002547f048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress                                int64\n",
      "Duration (in seconds)                   int64\n",
      "Finished                                 bool\n",
      "Personal_Income                       float64\n",
      "Period_Pain_Level                      object\n",
      "                                       ...   \n",
      "Primary_Insurance_Unknown                bool\n",
      "Period_Flow_Description_Light            bool\n",
      "Period_Flow_Description_Moderate         bool\n",
      "Period_Flow_Description_Very Heavy       bool\n",
      "Period_Flow_Description_Very Light       bool\n",
      "Length: 124, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check if everything is now numeric\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "71fc4b77-2c21-4781-a0e7-7ccee6073cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Slight pain' 'Severe pain' 'Moderate pain' 'No pain' 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Period_Pain_Level\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "74cce4f3-42f6-41ad-ac5c-91d8029cb9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period_Pain_Level converted to numeric values!\n"
     ]
    }
   ],
   "source": [
    "pain_mapping = {\n",
    "    \"No pain\": 0,\n",
    "    \"Slight pain\": 1,\n",
    "    \"Moderate pain\": 2,\n",
    "    \"Severe pain\": 3,\n",
    "    \"Unknown\": np.nan  # Handle \"Unknown\" as missing data\n",
    "}\n",
    "\n",
    "df[\"Period_Pain_Level\"] = df[\"Period_Pain_Level\"].map(pain_mapping)\n",
    "\n",
    "# Fill any missing values in the target column\n",
    "df[\"Period_Pain_Level\"].fillna(df[\"Period_Pain_Level\"].median(), inplace=True)\n",
    "\n",
    "print(\"Period_Pain_Level converted to numeric values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "2441dc97-4d49-4e87-801e-f2fa583b21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 3. 2. 0.]\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Period_Pain_Level\"].unique())  \n",
    "print(df.dtypes[\"Period_Pain_Level\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "0c8f5c4c-36e3-433b-883f-9148546bbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress                              0\n",
      "Duration (in seconds)                 0\n",
      "Finished                              0\n",
      "Personal_Income                       1\n",
      "Period_Pain_Level                     0\n",
      "                                     ..\n",
      "Primary_Insurance_Unknown             0\n",
      "Period_Flow_Description_Light         0\n",
      "Period_Flow_Description_Moderate      0\n",
      "Period_Flow_Description_Very Heavy    0\n",
      "Period_Flow_Description_Very Light    0\n",
      "Length: 124, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "ca1de3c3-d504-4585-aa98-6252cbb3293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values left in dataset: 22985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"Personal_Income\"].fillna(df[\"Personal_Income\"].median(), inplace=True)\n",
    "\n",
    "# Confirm missing values are gone\n",
    "print(\"Total missing values left in dataset:\", df.isnull().sum().sum())  # Should print \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "8efb5698-5e46-41a7-bdeb-f97f3ffd93f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoided_Social_Activities       545\n",
      "Concern_Level_About_Bleeding      5\n",
      "Chronic_Pelvic_Pain              90\n",
      "Q19                             545\n",
      "Q20                             545\n",
      "Q21                             545\n",
      "Q22                             545\n",
      "Q23                             545\n",
      "Q24                             545\n",
      "Q25                             545\n",
      "Q26                             545\n",
      "17                              545\n",
      "19                              545\n",
      "20                              545\n",
      "21                              545\n",
      "22                              545\n",
      "23                              545\n",
      "24                              545\n",
      "25                              545\n",
      "27                              545\n",
      "28                              545\n",
      "29                              545\n",
      "30                              545\n",
      "31                              545\n",
      "32                              545\n",
      "33                              545\n",
      "34                              545\n",
      "35                              545\n",
      "36                              545\n",
      "37                              545\n",
      "38                              545\n",
      "39                              545\n",
      "40                              545\n",
      "41                              545\n",
      "42                              545\n",
      "43                              545\n",
      "44                              545\n",
      "45                              545\n",
      "46                              545\n",
      "47                              545\n",
      "48                              545\n",
      "49                              545\n",
      "Q41                             545\n",
      "Q43                             545\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display columns with missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "acabb91f-8587-4eff-9154-49a55c9d227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values with median.\n"
     ]
    }
   ],
   "source": [
    "df.fillna(df.median(), inplace=True)\n",
    "print(\"Filled missing values with median.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "a16aa782-00ef-4204-93ad-95b40dfb49ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values left in dataset: 22890\n"
     ]
    }
   ],
   "source": [
    "print(\"Total missing values left in dataset:\", df.isnull().sum().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "55423be8-9a53-4849-9ad0-da2f3524181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoided_Social_Activities    545\n",
      "Q19                          545\n",
      "Q20                          545\n",
      "Q21                          545\n",
      "Q22                          545\n",
      "Q23                          545\n",
      "Q24                          545\n",
      "Q25                          545\n",
      "Q26                          545\n",
      "17                           545\n",
      "19                           545\n",
      "20                           545\n",
      "21                           545\n",
      "22                           545\n",
      "23                           545\n",
      "24                           545\n",
      "25                           545\n",
      "27                           545\n",
      "28                           545\n",
      "29                           545\n",
      "30                           545\n",
      "31                           545\n",
      "32                           545\n",
      "33                           545\n",
      "34                           545\n",
      "35                           545\n",
      "36                           545\n",
      "37                           545\n",
      "38                           545\n",
      "39                           545\n",
      "40                           545\n",
      "41                           545\n",
      "42                           545\n",
      "43                           545\n",
      "44                           545\n",
      "45                           545\n",
      "46                           545\n",
      "47                           545\n",
      "48                           545\n",
      "49                           545\n",
      "Q41                          545\n",
      "Q43                          545\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display only columns with missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "f28f5999-cf34-4e8a-a7c2-4248264c98dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted all columns to their correct data types.\n"
     ]
    }
   ],
   "source": [
    "# Convert numeric columns to float\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "\n",
    "print(\"Converted all columns to their correct data types.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "7f7bef56-5ee5-4168-8660-0948ea3fe1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoided_Social_Activities    545\n",
      "Q19                          545\n",
      "Q20                          545\n",
      "Q21                          545\n",
      "Q22                          545\n",
      "Q23                          545\n",
      "Q24                          545\n",
      "Q25                          545\n",
      "Q26                          545\n",
      "17                           545\n",
      "19                           545\n",
      "20                           545\n",
      "21                           545\n",
      "22                           545\n",
      "23                           545\n",
      "24                           545\n",
      "25                           545\n",
      "27                           545\n",
      "28                           545\n",
      "29                           545\n",
      "30                           545\n",
      "31                           545\n",
      "32                           545\n",
      "33                           545\n",
      "34                           545\n",
      "35                           545\n",
      "36                           545\n",
      "37                           545\n",
      "38                           545\n",
      "39                           545\n",
      "40                           545\n",
      "41                           545\n",
      "42                           545\n",
      "43                           545\n",
      "44                           545\n",
      "45                           545\n",
      "46                           545\n",
      "47                           545\n",
      "48                           545\n",
      "49                           545\n",
      "Q41                          545\n",
      "Q43                          545\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display only columns with missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "ffd006d8-7ac7-4e52-937f-8eb1767b3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns: Index(['Progress', 'Duration (in seconds)', 'Personal_Income',\n",
      "       'Period_Pain_Level', 'Work_School_Impact_Days',\n",
      "       'Missed_Work_School_Days', 'Avoided_Social_Activities',\n",
      "       'Concern_Level_About_Bleeding', 'Chronic_Pelvic_Pain', 'Q19', 'Q20',\n",
      "       'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', '17', '19', '20', '21', '22',\n",
      "       '23', '24', '25', '27', '28', '29', '30', '31', '32', '33', '34', '35',\n",
      "       '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47',\n",
      "       '48', '49', 'Q41', 'Q43', 'Need to use double sanitary protection',\n",
      "       'Soak through one or more sanitary pads or tampons every hour',\n",
      "       'Bleed for longer than a week',\n",
      "       'Pass blood clots larger than a quarter',\n",
      "       'Restrict daily activities due to heavy menstrual flow',\n",
      "       'Need to wake up to change sanitary protection during the night'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "print(\"Numeric Columns:\", numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "5ce95f40-dc91-4828-b40d-1f9306e67cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values in all numeric columns with the median.\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in all numeric columns with their median\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "print(\"Filled missing values in all numeric columns with the median.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "629783f5-dd3e-42df-942f-e8f200b2511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoided_Social_Activities    545\n",
      "Q19                          545\n",
      "Q20                          545\n",
      "Q21                          545\n",
      "Q22                          545\n",
      "Q23                          545\n",
      "Q24                          545\n",
      "Q25                          545\n",
      "Q26                          545\n",
      "17                           545\n",
      "19                           545\n",
      "20                           545\n",
      "21                           545\n",
      "22                           545\n",
      "23                           545\n",
      "24                           545\n",
      "25                           545\n",
      "27                           545\n",
      "28                           545\n",
      "29                           545\n",
      "30                           545\n",
      "31                           545\n",
      "32                           545\n",
      "33                           545\n",
      "34                           545\n",
      "35                           545\n",
      "36                           545\n",
      "37                           545\n",
      "38                           545\n",
      "39                           545\n",
      "40                           545\n",
      "41                           545\n",
      "42                           545\n",
      "43                           545\n",
      "44                           545\n",
      "45                           545\n",
      "46                           545\n",
      "47                           545\n",
      "48                           545\n",
      "49                           545\n",
      "Q41                          545\n",
      "Q43                          545\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show only columns with missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "b3a4a36e-dffb-483f-b491-62d9db715dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced all missing-value columns to numeric type.\n"
     ]
    }
   ],
   "source": [
    "# Convert all these columns to numeric type (forcing conversion)\n",
    "missing_cols = [\n",
    "    \"Avoided_Social_Activities\", \"Q19\", \"Q20\", \"Q21\", \"Q22\", \"Q23\", \"Q24\", \"Q25\", \"Q26\", \"17\", \"19\", \"20\", \"21\",\n",
    "    \"22\", \"23\", \"24\", \"25\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\",\n",
    "    \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"Q41\", \"Q43\"\n",
    "]\n",
    "\n",
    "\n",
    "df[missing_cols] = df[missing_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Forced all missing-value columns to numeric type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "b53a584a-a71d-44a3-97e1-3cc467aae757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully filled missing values in all previously stuck columns!\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with the median (since they are now numeric)\n",
    "df[missing_cols] = df[missing_cols].fillna(df[missing_cols].median())\n",
    "\n",
    "print(\"Successfully filled missing values in all previously stuck columns!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "266656e2-88aa-4145-a663-dfac2e9a15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values left in dataset: 22890\n"
     ]
    }
   ],
   "source": [
    "print(\"Total missing values left in dataset:\", df.isnull().sum().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "1a4b9c00-e1cb-427e-8606-3709fa91d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoided_Social_Activities    100.0\n",
      "Q19                          100.0\n",
      "Q20                          100.0\n",
      "Q21                          100.0\n",
      "Q22                          100.0\n",
      "Q23                          100.0\n",
      "Q24                          100.0\n",
      "Q25                          100.0\n",
      "Q26                          100.0\n",
      "17                           100.0\n",
      "19                           100.0\n",
      "20                           100.0\n",
      "21                           100.0\n",
      "22                           100.0\n",
      "23                           100.0\n",
      "24                           100.0\n",
      "25                           100.0\n",
      "27                           100.0\n",
      "28                           100.0\n",
      "29                           100.0\n",
      "30                           100.0\n",
      "31                           100.0\n",
      "32                           100.0\n",
      "33                           100.0\n",
      "34                           100.0\n",
      "35                           100.0\n",
      "36                           100.0\n",
      "37                           100.0\n",
      "38                           100.0\n",
      "39                           100.0\n",
      "40                           100.0\n",
      "41                           100.0\n",
      "42                           100.0\n",
      "43                           100.0\n",
      "44                           100.0\n",
      "45                           100.0\n",
      "46                           100.0\n",
      "47                           100.0\n",
      "48                           100.0\n",
      "49                           100.0\n",
      "Q41                          100.0\n",
      "Q43                          100.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find percentage of missing values in each column\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Show columns with more than 10% missing values\n",
    "print(missing_percentages[missing_percentages > 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "840b6f07-9a29-421b-8433-b59ad51d0593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped all columns with 100% missing values.\n"
     ]
    }
   ],
   "source": [
    "# Drop all columns that are 100% missing\n",
    "df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "print(\"Dropped all columns with 100% missing values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "50442435-ef8d-4a1e-81db-2195f3541fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values left in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Total missing values left in dataset:\", df.isnull().sum().sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "b3cc9ece-323a-4cbb-81e5-70f7c7aeee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work_School_Impact_Days         object\n",
      "Concern_Level_About_Bleeding    object\n",
      "Personal_Income                 object\n",
      "Missed_Work_School_Days         object\n",
      "Heavy_Bleeding_Indicators       object\n",
      "dtype: object\n",
      "\n",
      "Non-numeric columns: Index(['Work_School_Impact_Days', 'Concern_Level_About_Bleeding',\n",
      "       'Personal_Income', 'Missed_Work_School_Days',\n",
      "       'Heavy_Bleeding_Indicators'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check if any column in X_train contains text\n",
    "print(X_train.dtypes)\n",
    "\n",
    "\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nNon-numeric columns:\", non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "258a1cab-a48a-4919-91f6-642dfd536145",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[588], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m non_numeric_cols:\n\u001b[0;32m      6\u001b[0m     X_train[col] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(X_train[col])\n\u001b[1;32m----> 7\u001b[0m     X_test[col] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(X_test[col])  \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReapplied encoding to categorical columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'Soak through one or more sanitary pads or tampons every hour for several consecutive hours,Pass blood clots larger than a quarter,Restrict daily activities due to heavy menstrual flow'"
     ]
    }
   ],
   "source": [
    "# Convert all categorical columns into numeric format (if needed)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in non_numeric_cols:\n",
    "    X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    X_test[col] = label_encoder.transform(X_test[col])  \n",
    "\n",
    "print(\"Reapplied encoding to categorical columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "b39de089-0cd6-41a3-8bcd-878b6c5afedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully applied One-Hot Encoding. Both train and test sets now have matching columns.\n"
     ]
    }
   ],
   "source": [
    "# Ensure categorical columns are correctly identified\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Apply One-Hot Encoding to both train and test sets\n",
    "X_train = pd.get_dummies(X_train, columns=non_numeric_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=non_numeric_cols, drop_first=True)\n",
    "\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(\"Successfully applied One-Hot Encoding. Both train and test sets now have matching columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "0b360478-0033-4a16-832c-a0b444a58ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36256\\3401694312.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my_pred_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Evaluate performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0maccuracy_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_rf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \"\"\"\n\u001b[1;32m--> 820\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \"\"\"\n\u001b[0;32m    860\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_partition_estimators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[0;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    876\u001b[0m                         )\n\u001b[0;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m                 raise ValueError(\n\u001b[0;32m    882\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mxp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"numpy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"numpy.array_api\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         if (\n\u001b[0;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Unknown'"
     ]
    }
   ],
   "source": [
    "# Train a new Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Improved Random Forest Accuracy: {accuracy_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Improved Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "c00b8eb3-cd7c-4766-8759-36ecc24890da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-numeric columns in X_test: Index(['Heavy_Bleeding_Indicators'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify non-numeric columns in X_test\n",
    "non_numeric_cols = X_test.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nNon-numeric columns in X_test:\", non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "aa54005d-c67b-4193-920c-b24823cc21f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted `Heavy_Bleeding_Indicators` into separate numeric columns.\n"
     ]
    }
   ],
   "source": [
    "# Define the list of possible symptoms\n",
    "symptoms = [\n",
    "    \"Need to use double sanitary protection\",\n",
    "    \"Soak through one or more sanitary pads or tampons every hour\",\n",
    "    \"Bleed for longer than a week\",\n",
    "    \"Pass blood clots larger than a quarter\",\n",
    "    \"Restrict daily activities due to heavy menstrual flow\",\n",
    "    \"Need to wake up to change sanitary protection during the night\"\n",
    "]\n",
    "\n",
    "# Create new binary columns for each symptom\n",
    "for symptom in symptoms:\n",
    "    X_train[symptom] = X_train[\"Heavy_Bleeding_Indicators\"].apply(lambda x: 1 if symptom in str(x) else 0)\n",
    "    X_test[symptom] = X_test[\"Heavy_Bleeding_Indicators\"].apply(lambda x: 1 if symptom in str(x) else 0)\n",
    "\n",
    "# Drop the original text column\n",
    "X_train.drop(columns=[\"Heavy_Bleeding_Indicators\"], inplace=True)\n",
    "X_test.drop(columns=[\"Heavy_Bleeding_Indicators\"], inplace=True)\n",
    "\n",
    "print(\"Converted `Heavy_Bleeding_Indicators` into separate numeric columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "b921c8de-2f8d-41c7-b646-e0770669e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining non-numeric columns in X_test: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining non-numeric columns\n",
    "non_numeric_cols_after = X_test.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nRemaining non-numeric columns in X_test:\", non_numeric_cols_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "f3247060-7c18-43bd-b23b-e2f230838c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Random Forest Accuracy: 0.35\n",
      "\n",
      "Improved Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Moderate pain       0.41      0.24      0.30        46\n",
      "      No pain       0.17      0.67      0.27         3\n",
      "  Severe pain       0.33      0.38      0.35        24\n",
      "  Slight pain       0.37      0.44      0.41        36\n",
      "\n",
      "     accuracy                           0.35       109\n",
      "    macro avg       0.32      0.43      0.33       109\n",
      " weighted avg       0.37      0.35      0.35       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a new Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Improved Random Forest Accuracy: {accuracy_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Improved Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "54d49b98-8656-4bfd-a9bc-86abdceda81e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 1, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[595], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      4\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplied SMOTE to balance class distribution in training data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:364\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[1;32m--> 364\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    365\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[0;32m    366\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:810\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    808\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    815\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    816\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 1, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applied SMOTE to balance class distribution in training data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "2ba93b13-34e4-4550-8b52-12913007b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period_Pain_Level\n",
      "Moderate pain    180\n",
      "Slight pain      151\n",
      "Severe pain       78\n",
      "No pain           26\n",
      "Unknown            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "d2b44363-0415-4dee-8e94-cef947b97a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed the 'Unknown' class from training data.\n"
     ]
    }
   ],
   "source": [
    "# Drop \"Unknown\" from y_train and corresponding X_train rows\n",
    "mask = y_train != \"Unknown\"\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(\"Removed the 'Unknown' class from training data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "fc727854-6b36-46fb-b3ce-4f5749e1569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied SMOTE successfully with adjusted k_neighbors.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applied SMOTE successfully with adjusted k_neighbors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "d7a0b036-7630-4a8b-8e39-9a33cc261799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Random Forest Accuracy: 0.32\n",
      "\n",
      "Improved Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Moderate pain       0.46      0.28      0.35        46\n",
      "      No pain       0.07      0.33      0.11         3\n",
      "  Severe pain       0.36      0.33      0.35        24\n",
      "  Slight pain       0.30      0.36      0.33        36\n",
      "\n",
      "     accuracy                           0.32       109\n",
      "    macro avg       0.30      0.33      0.28       109\n",
      " weighted avg       0.38      0.32      0.34       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the optimized Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Improved Random Forest Accuracy: {accuracy_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Improved Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "d2ef7635-4b74-4e66-bbe9-ec07685645bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (No SMOTE): 0.34\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Moderate pain       0.41      0.24      0.30        46\n",
      "      No pain       0.15      0.67      0.25         3\n",
      "  Severe pain       0.33      0.38      0.35        24\n",
      "  Slight pain       0.36      0.42      0.38        36\n",
      "\n",
      "     accuracy                           0.34       109\n",
      "    macro avg       0.31      0.42      0.32       109\n",
      " weighted avg       0.37      0.34      0.34       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest without SMOTE, using class_weight instead\n",
    "rf_model = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy (No SMOTE): {accuracy_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "80b35b61-3c15-4b7d-b480-ed7dcdc68ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Ranking:\n",
      "\n",
      "Concern_Level_About_Bleeding                                      0.444277\n",
      "Work_School_Impact_Days                                           0.210514\n",
      "Personal_Income                                                   0.200079\n",
      "Missed_Work_School_Days                                           0.145130\n",
      "Need to use double sanitary protection                            0.000000\n",
      "Soak through one or more sanitary pads or tampons every hour      0.000000\n",
      "Bleed for longer than a week                                      0.000000\n",
      "Pass blood clots larger than a quarter                            0.000000\n",
      "Restrict daily activities due to heavy menstrual flow             0.000000\n",
      "Need to wake up to change sanitary protection during the night    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "print(\"Feature Importance Ranking:\\n\")\n",
    "print(feature_importances.head(20))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "d54ceac0-0b37-41c1-af2a-53327d340946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed non-informative features. Using only the most important ones.\n"
     ]
    }
   ],
   "source": [
    "# Keep only the top features (remove features with 0 importance)\n",
    "top_features = [\n",
    "    \"Concern_Level_About_Bleeding\",\n",
    "    \"Work_School_Impact_Days\",\n",
    "    \"Personal_Income\",\n",
    "    \"Missed_Work_School_Days\"\n",
    "]\n",
    "\n",
    "X_train_filtered = X_train[top_features]\n",
    "X_test_filtered = X_test[top_features]\n",
    "\n",
    "print(\"Removed non-informative features. Using only the most important ones.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "b4d99bbe-c32e-48d5-a68b-f983442da6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Random Forest Accuracy: 0.35\n",
      "\n",
      "Improved Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Moderate pain       0.42      0.24      0.31        46\n",
      "      No pain       0.15      0.67      0.25         3\n",
      "  Severe pain       0.33      0.38      0.35        24\n",
      "  Slight pain       0.37      0.44      0.41        36\n",
      "\n",
      "     accuracy                           0.35       109\n",
      "    macro avg       0.32      0.43      0.33       109\n",
      " weighted avg       0.38      0.35      0.35       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest with filtered features\n",
    "rf_model = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_model.fit(X_train_filtered, y_train)  # Train with filtered dataset\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_filtered)  \n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Improved Random Forest Accuracy: {accuracy_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Improved Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "c1d75e22-7572-419a-8dba-016f1e7955a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got ['Moderate pain' 'No pain' 'Severe pain' 'Slight pain']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[604], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train XGBoost with filtered features\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train_filtered, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test_filtered)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1559\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1556\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1558\u001b[0m ):\n\u001b[1;32m-> 1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m     )\n\u001b[0;32m   1564\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got ['Moderate pain' 'No pain' 'Severe pain' 'Slight pain']"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train XGBoost with filtered features\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(X_train_filtered, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.2f}\\n\")\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "edb15531-8d4a-4505-be6e-d2b230b63f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\jacquelynhopkins\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\jacquelynhopkins\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\jacquelynhopkins\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "8e6d190b-6afd-4133-aadc-2665103b276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost is installed and ready to use!\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(\"XGBoost is installed and ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "5a20a4b6-5e50-4588-9e9f-0df5c4d199ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got ['Moderate pain' 'No pain' 'Severe pain' 'Slight pain']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[607], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train XGBoost with filtered features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train_filtered, y_train)  \u001b[38;5;66;03m# Train on your dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test_filtered)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1559\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1556\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1558\u001b[0m ):\n\u001b[1;32m-> 1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m     )\n\u001b[0;32m   1564\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got ['Moderate pain' 'No pain' 'Severe pain' 'Slight pain']"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train XGBoost with filtered features\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(X_train_filtered, y_train)  # Train on your dataset\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.2f}\\n\")\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "7c8ff019-dad4-4a3e-abc3-8d68364da81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted `y_train` and `y_test` to numeric values:\n",
      "{'Moderate pain': 0, 'No pain': 1, 'Severe pain': 2, 'Slight pain': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert y_train and y_test to numeric values\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Converted `y_train` and `y_test` to numeric values:\")\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "751f1297-b3ff-4074-8620-8d4a72b41bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.40\n",
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.46      0.49        46\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.35      0.25      0.29        24\n",
      "           3       0.38      0.47      0.42        36\n",
      "\n",
      "    accuracy                           0.40       109\n",
      "   macro avg       0.31      0.29      0.30       109\n",
      "weighted avg       0.42      0.40      0.41       109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JacquelynHopkins\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:06:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost with encoded labels\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(X_train_filtered, y_train_encoded)  # Use encoded labels\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.2f}\\n\")\n",
    "\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "95ef437a-31dd-49bb-b15e-b2e32e876366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy (with Class Weighting): 0.40\n",
      "\n",
      "XGBoost Classification Report (with Class Weighting):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.46      0.49        46\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.35      0.25      0.29        24\n",
      "           3       0.38      0.47      0.42        36\n",
      "\n",
      "    accuracy                           0.40       109\n",
      "   macro avg       0.31      0.29      0.30       109\n",
      "weighted avg       0.42      0.40      0.41       109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JacquelynHopkins\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:06:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get class distribution in y_train_encoded\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = dict(Counter(y_train_encoded))\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "\n",
    "scale_weights = {k: total_samples/v for k, v in class_counts.items()}\n",
    "scale_pos_weight = [scale_weights[i] for i in sorted(scale_weights.keys())]\n",
    "\n",
    "xgb_model_balanced = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\",\n",
    "                                   scale_pos_weight=scale_pos_weight, random_state=42)\n",
    "xgb_model_balanced.fit(X_train_filtered, y_train_encoded)\n",
    "\n",
    "\n",
    "y_pred_balanced = xgb_model_balanced.predict(X_test_filtered)\n",
    "\n",
    "accuracy_balanced = accuracy_score(y_test_encoded, y_pred_balanced)\n",
    "print(f\"XGBoost Accuracy (with Class Weighting): {accuracy_balanced:.2f}\\n\")\n",
    "\n",
    "print(\"XGBoost Classification Report (with Class Weighting):\")\n",
    "print(classification_report(y_test_encoded, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "61ca9bb9-9aa5-4e09-8266-12fbc4d6f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JacquelynHopkins\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:07:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\JacquelynHopkins\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:07:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned XGBoost Accuracy: 0.50\n",
      "\n",
      "Best Tuned XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59        46\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.57      0.33      0.42        24\n",
      "           3       0.50      0.39      0.44        36\n",
      "\n",
      "    accuracy                           0.50       109\n",
      "   macro avg       0.39      0.36      0.36       109\n",
      "weighted avg       0.50      0.50      0.49       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300, 500],  \n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  \n",
    "    \"max_depth\": [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42), \n",
    "                           param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_filtered, y_train_encoded)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_xgb.fit(X_train_filtered, y_train_encoded)\n",
    "\n",
    "\n",
    "y_pred_best_xgb = best_xgb.predict(X_test_filtered)\n",
    "\n",
    "\n",
    "accuracy_best_xgb = accuracy_score(y_test_encoded, y_pred_best_xgb)\n",
    "print(f\"Best Tuned XGBoost Accuracy: {accuracy_best_xgb:.2f}\\n\")\n",
    "\n",
    "print(\"Best Tuned XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_best_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "50b99b33-3795-44fa-bd76-35944b064db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only top 4 features.\n",
      "Optimized XGBoost Accuracy (Top Features Only): 0.50\n",
      "\n",
      " Optimized XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59        46\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.57      0.33      0.42        24\n",
      "           3       0.50      0.39      0.44        36\n",
      "\n",
      "    accuracy                           0.50       109\n",
      "   macro avg       0.39      0.36      0.36       109\n",
      "weighted avg       0.50      0.50      0.49       109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JacquelynHopkins\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:07:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the top 4 important features\n",
    "top_features = [\"Work_School_Impact_Days\", \"Missed_Work_School_Days\", \n",
    "                \"Concern_Level_About_Bleeding\", \"Personal_Income\"]\n",
    "\n",
    "# Create new datasets\n",
    "X_train_optimized = X_train_filtered[top_features]\n",
    "X_test_optimized = X_test_filtered[top_features]\n",
    "\n",
    "print(f\"Keeping only top {len(top_features)} features.\")\n",
    "\n",
    "# Retrain XGBoost with optimized features\n",
    "best_xgb.fit(X_train_optimized, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_optimized = best_xgb.predict(X_test_optimized)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_optimized = accuracy_score(y_test_encoded, y_pred_optimized)\n",
    "print(f\"Optimized XGBoost Accuracy (Top Features Only): {accuracy_optimized:.2f}\\n\")\n",
    "\n",
    "print(\" Optimized XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "3c02add1-947e-458b-b759-b2fa4d4592d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been saved! You can now use it anytime.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best XGBoost model\n",
    "joblib.dump(best_xgb, \"final_xgboost_model.pkl\")\n",
    "\n",
    "print(\"The best model has been saved! You can now use it anytime.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "308ac9e6-a0f7-4663-942e-3789f7937a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy Summary:\n",
      "Random Forest Accuracy: 0.35\n",
      "Logistic Regression Accuracy: 0.25\n",
      "XGBoost (Optimized): 0.50\n",
      "\n",
      "**Best Model:** XGBoost (Optimized) with 0.50 accuracy\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy comparison\n",
    "print(\"\\nModel Accuracy Summary:\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_log:.2f}\")\n",
    "print(f\"XGBoost (Optimized): {accuracy_optimized:.2f}\")  \n",
    "\n",
    "\n",
    "model_accuracies = {\n",
    "    \"Random Forest\": accuracy_rf,\n",
    "    \"Logistic Regression\": accuracy_log,\n",
    "    \"XGBoost (Optimized)\": accuracy_optimized  \n",
    "}\n",
    "\n",
    "\n",
    "best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
    "best_model_accuracy = model_accuracies[best_model_name]\n",
    "\n",
    "\n",
    "print(f\"\\n**Best Model:** {best_model_name} with {best_model_accuracy:.2f} accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "c5545b3e-ad8f-4a0f-aef4-3c7744f50ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ©¸ **Predicted Pain Levels:** ['Severe pain', 'Moderate pain', 'Slight pain']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(\"final_xgboost_model.pkl\")\n",
    "\n",
    "# Example: New data for prediction\n",
    "new_data = pd.DataFrame({\n",
    "    \"Work_School_Impact_Days\": [3, 1, 5],  \n",
    "    \"Missed_Work_School_Days\": [2, 0, 3],\n",
    "    \"Concern_Level_About_Bleeding\": [4, 1, 3],\n",
    "    \"Personal_Income\": [50000, 30000, 75000]\n",
    "})\n",
    "\n",
    "\n",
    "predictions = loaded_model.predict(new_data)\n",
    "\n",
    "\n",
    "label_mapping = {0: \"Moderate pain\", 1: \"No pain\", 2: \"Severe pain\", 3: \"Slight pain\"}\n",
    "predicted_labels = [label_mapping[p] for p in predictions]\n",
    "\n",
    "print(\"ðŸ©¸ **Predicted Pain Levels:**\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec51f5-a703-4ae9-9d38-d831e79289b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
